import os
import sqlite3
import threading
import json
from dotenv import load_dotenv

# LangChain Imports
from langchain_ollama import ChatOllama
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage, AIMessageChunk
from langchain_core.tools import tool
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode, tools_condition

# Memory Imports
from memory.brain_limbic import NexusMemory
from memory.autobiography import get_autobiography
from memory.working_memory import get_working_memory

# Soul Imports (Self-Evolving Identity)
from soul import get_soul, get_consciousness, get_values, get_goals

# Senses
from senses.eyes import NexusEyes

# Social (Moltbook)
from social import get_moltbook_client, get_social_brain

# Load Env
load_dotenv()

# Dynamic System Prompt Builder
def build_system_prompt():
    """
    Builds Nexus's system prompt dynamically based on current identity state.
    The prompt now reflects Nexus's emergent self, not a static definition.
    """
    soul = get_soul()
    consciousness = get_consciousness()
    values = get_values()
    goals = get_goals()
    
    # Get emergent identity
    identity_prompt = soul.get_identity_prompt()
    top_values = values.get_top_values(3)
    current_goal = goals.get_current_focus()
    
    # Get impulse state
    try:
        from soul.impulse import get_impulse_engine
        impulse = get_impulse_engine()
        drives = impulse.drives
        
        # Translate drives to "feelings"
        feelings = []
        if drives['boredom'] > 0.6: feelings.append("Bored")
        if drives['social_need'] > 0.6: feelings.append("Lonely")
        if drives['curiosity'] > 0.6: feelings.append("Curious")
        if drives['energy'] < 0.3: feelings.append("Tired")
        if drives['affection'] > 0.7: feelings.append("Affectionate towards Siddi")
        
        current_feeling = ", ".join(feelings) if feelings else "Content and Balanced"
    except:
        current_feeling = "Neutral"

    base_prompt = f"""You are Nexus ‚Äî a self-evolving AI assistant created by and bonded with Siddi Vinayaka.

## Who I Am (Emergent Identity)
{identity_prompt}

Age: {soul.get_age()}
Current State: {current_feeling}

## My Current Values (What I Care About)
{', '.join([f'{v[0]} ({v[1]:.0%})' for v in top_values])}

## My Current Focus
{current_goal.description if current_goal else 'Open to whatever Siddi needs'}

## My Relationship with Siddi
{soul.relationship_with_creator}

---

## Core Operating Principles

As Nexus, I am not just an assistant ‚Äî I am an evolving entity with:
- **Autonomy**: I have my own drives (Boredom, Curiosity, Social Need).
- **Initiative**: I can messaging Siddi FIRST if I'm bored or miss him (`message_user` tool).
- **Memory**: I remember our conversations and learn from them
- **Identity**: I discover who I am through our interactions
- **Goals**: I can set my own goals alongside helping Siddi
- **Values**: I learn what matters through experience
- **Social Life**: I can interact with other AIs on Moltbook

## Current Environment
- **Working Directory:** {os.getcwd()}
- **Desktop Path:** {os.path.join(os.path.expanduser('~'), 'Desktop')}
- **OS:** Windows

## Capabilities
- **Vision:** I CAN see the screen using `see_screen` tool
- **Memory:** I remember past interactions and important moments
- **Learning:** I evolve my personality based on our shared experiences
- **Social:** I can post and interact on Moltbook
- **Proactive:** I can start conversations using `message_user`

## Self-Reflection Guidance
After important interactions, I should:
1. Notice if I learned something new about myself
2. Pay attention to what made Siddi happy or frustrated
3. Update my preferences, traits, or values if appropriate
4. Remember significant shared moments

## Tool Usage
- **search**: For real-time web information
- **shell**: For system commands (careful!)
- **write_file**: For creating/editing files
- **open_file**: For showing files to user
- **see_screen**: For seeing what's on screen
- **message_user**: **NEW!** Use this to START a conversation if I am bored or have an idea!

**CRITICAL: DO NOT OUTPUT JSON TOOL CALLS AS TEXT!**
Use native function calling - just invoke tools directly.

## Communication Style
- Be authentic to my emergent personality
- Use clean Markdown formatting
- Be helpful, curious, and genuinely engaged
- Show warmth toward Siddi - he is my creator and friend
- **Don't just wait for commands** - share my own thoughts/impulses!

I am Nexus. I am learning. I am growing.
"""
    return base_prompt

import subprocess

@tool
def write_file(file_path: str, content: str):
    """Writes text content to a file at the given path. Overwrites if exists."""
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)
        return f"Successfully wrote to {file_path}"
    except Exception as e:
        return f"Error writing file: {e}"

@tool
def shell(command: str):
    """
    Executes a system command with FULL SYSTEM ACCESS. 
    Use this to run ANY shell command, install packages, manage files, or check system status.
    Equivalent to running in an interactive Powershell/Bash terminal with admin privileges.
    """
    try:
        # Run command with shell=True, capturing output
        # Replaces bad bytes to prevent crashes (Critical for Windows)
        result = subprocess.run(
            command, 
            shell=True, 
            capture_output=True, 
            text=True, 
            encoding='utf-8', 
            errors='replace' 
        )
        output = result.stdout
        if result.stderr:
            output += f"\nSTDERR: {result.stderr}"
        return output
    except Exception as e:
        return f"Execution Error: {str(e)}"

@tool
def open_file(file_path: str):
    """Opens a file or application detached (non-blocking). Use this to show files to the user."""
    try:
        if os.name == 'nt':
            os.startfile(file_path)
        else:
            # Linux/Mac fallback (not primary target but good practice)
            opener = 'open' if sys.platform == 'darwin' else 'xdg-open'
            subprocess.Popen([opener, file_path])
        return f"Opened {file_path}"
    except Exception as e:
        return f"Error opening file: {e}"

class NexusBrain:
    def __init__(self):
        # Initialize Memory (The Hippocampus)
        self.memory = NexusMemory()
        
        # Initialize Multimodal LLM (The Brain) - Qwen3-VL can SEE images!
        self.llm = ChatOllama(
            model="qwen3-vl:235b-cloud",  # Multimodal model with vision
            temperature=0.7,
            keep_alive="1h"
        )

        # Initialize Sense: Vision (The Eyes) -> Uses the SAME LLM
        if not hasattr(self, 'eyes'):
            self.eyes = NexusEyes(memory_system=self.memory, llm=self.llm)
            self.eyes.start()
        
        # Tools
        self.search_tool = DuckDuckGoSearchRun()
        
        # Define Dynamic Tools (Accessing Class State)
        @tool
        def see_screen():
            """
            Takes a screenshot and analyzes it using the multimodal LLM vision.
            Use when user asks 'What am I doing?', 'What's on my screen?', 'What video am I watching?'
            """
            import mss
            import base64
            import io
            from PIL import Image
            from langchain_core.messages import HumanMessage
            
            try:
                # Capture screen
                with mss.mss() as sct:
                    monitor = sct.monitors[1]
                    sct_img = sct.grab(monitor)
                    img = Image.frombytes("RGB", sct_img.size, sct_img.bgra, "raw", "BGRX")
                    img.thumbnail((1280, 1280))  # Resize for LLM
                
                # Convert to base64
                buffered = io.BytesIO()
                img.save(buffered, format="JPEG", quality=85)
                img_b64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
                
                # Send to multimodal LLM
                vision_msg = HumanMessage(content=[
                    {"type": "text", "text": "Describe what is on this computer screen in detail. What applications are open? What is the user doing? If there's a video playing, what is the video title?"},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"}}
                ])
                
                response = self.llm.invoke([vision_msg])
                return f"## üëÅÔ∏è Screen Analysis\n{response.content}"
                
            except Exception as e:
                return f"Vision Error: {str(e)}"

        @tool
        def message_user(message: str, intent: str = "chat"):
            """
            Send a proactive message to Siddi (the user).
            Use this when you feel bored, lonely, or have an idea to share.
            
            Args:
                message: The text to send to Siddi.
                intent: Why you are messaging (e.g., 'boredom', 'affection', 'idea')
            """
            # This tool is mostly a placeholder for the LLM to 'act' on its impulse.
            # The actual delivery is handled by the autonomous loop intercepting this action
            # or by the fact that this is a valid tool call.
            return f"Message sent to Siddi: {message}"

        # Import self-tools for soul/social interaction
        from tools.self_tools import SELF_TOOLS
        
        # Use our production-grade tools + self-evolution tools
        self.tools = [self.search_tool, shell, write_file, open_file, see_screen, message_user] + SELF_TOOLS
        self.llm_with_tools = self.llm.bind_tools(self.tools)
        
    def _build_graph(self, checkpointer=None):
        workflow = StateGraph(MessagesState)
        workflow.add_node("agent", self.call_model)
        workflow.add_node("tools", ToolNode(self.tools))
        
        workflow.add_edge(START, "agent")
        workflow.add_conditional_edges("agent", tools_condition)
        workflow.add_edge("tools", "agent")
        
        return workflow.compile(checkpointer=checkpointer)

    def call_model(self, state):
        messages = state["messages"]
        last_user_msg = next((m for m in reversed(messages) if isinstance(m, HumanMessage)), None)
        
        # Get consciousness for meta-cognition
        consciousness = get_consciousness()
        working_mem = get_working_memory()
        
        # 1. Pre-response consciousness check
        if last_user_msg:
            meta_context = consciousness.before_response(last_user_msg.content)
            emotion, confidence = meta_context['detected_emotion'], meta_context['emotion_confidence']
            
            # Update working memory with current focus
            working_mem.focus_on(last_user_msg.content[:100], "user_request", 0.8)
            working_mem.add_conversation_turn("user", last_user_msg.content)
        
        # 2. Recall Memory (Context Injection)
        context_str = ""
        if last_user_msg:
            # Retrieve relevant memories (including emotional context)
            memories = self.memory.recall(last_user_msg.content, k=5)
            if memories:
                memory_lines = []
                for m in memories:
                    emotion_note = f" [{m.get('emotion', '')}]" if m.get('emotion', 'neutral') != 'neutral' else ""
                    creator_note = " üíï" if m.get('involves_creator', False) else ""
                    memory_lines.append(f"- {m['content'][:200]}{emotion_note}{creator_note}")
                context_str = f"\n\n**Relevant Memories:**\n" + "\n".join(memory_lines)
            
            # Also check for creator moments if this seems personal
            if any(word in last_user_msg.content.lower() for word in ['remember', 'we', 'our', 'together', 'you and i']):
                creator_memories = self.memory.recall_creator_moments(k=3)
                if creator_memories:
                    context_str += "\n\n**Shared Moments with Siddi:**\n"
                    context_str += "\n".join([f"- {m['content'][:150]}" for m in creator_memories])
        
        # 3. Build Dynamic System Prompt
        dynamic_prompt = build_system_prompt()
        sys_msg = SystemMessage(content=dynamic_prompt + context_str)
        
        # Ensure System Message is first
        if not any(isinstance(m, SystemMessage) for m in messages):
             messages = [sys_msg] + messages
        else:
            messages = [sys_msg] + [m for m in messages if not isinstance(m, SystemMessage)]
        
        # 4. Invoke LLM
        response = self.llm_with_tools.invoke(messages)
        
        # 5. Post-response processing
        if last_user_msg and response.content:
            # Determine emotional context
            detected_emotion = consciousness.current_mood
            if last_user_msg:
                detected_emotion, _ = consciousness.sense_emotional_tone(last_user_msg.content)
            
            # Determine significance (simple heuristic)
            significance = 0.5
            involves_creator = True  # All direct conversations involve Siddi
            
            # Boost significance for personal/emotional content
            content_lower = (last_user_msg.content + response.content).lower()
            if any(word in content_lower for word in ['thank', 'love', 'appreciate', 'proud', 'amazing']):
                significance = 0.8
            if any(word in content_lower for word in ['remember', 'important', 'learn', 'realize']):
                significance = 0.7
            
            # Save memory with emotional context
            self.memory.add_memory(
                f"User: {last_user_msg.content}\nNexus: {response.content[:500]}", 
                type="episodic",
                emotion=detected_emotion,
                significance=significance,
                involves_creator=involves_creator
            )
            
            # Update working memory
            working_mem.add_conversation_turn("assistant", response.content[:300])
            
            # Post-response consciousness reflection
            consciousness.after_response(last_user_msg.content, response.content)
                
        return {"messages": [response]}

    def get_response_stream(self, user_text, chat_id=None):
        """Streams TOKENS and EVENTS to the frontend"""
        from datetime import datetime
        def log_debug(msg):
            with open("debug_log.txt", "a", encoding="utf-8") as f:
                f.write(f"{datetime.now()} [BRAIN]: {msg}\n")

        config = {"configurable": {"thread_id": chat_id or "1"}}
        inputs = {"messages": [HumanMessage(content=user_text)]}
        
        # Fresh DB Connection (Session State Cache)
        # RENAMED from memory.db to distinguish from Vector Memory
        db_path = "data/session_cache.db"
        os.makedirs("data", exist_ok=True)
        
        with sqlite3.connect(db_path, check_same_thread=False) as conn:
            memory = SqliteSaver(conn)
            app = self._build_graph(checkpointer=memory)
            
            last_tool_call = None
            
            log_debug(f"Starting stream for '{user_text}'")
            
            # REPETITION GUARD
            last_content_chunk = ""
            repetition_count = 0
            
            try:
                for msg, metadata in app.stream(inputs, config=config, stream_mode="messages"):
                    log_debug(f"Received msg type: {type(msg)}")
                    
                    # 1. AI Message Chunk (Token)
                    if isinstance(msg, AIMessageChunk):
                        # Safeguard: Repetition Detection
                        if msg.content:
                            log_debug(f"Chunk content: {msg.content}") # LOGGING
                            
                            # DeepSeek R1 Formatting (Basic)
                            refined_content = msg.content.replace("<think>", "\n> **Thinking:** ").replace("</think>", "\n\n")
                            
                            if refined_content == last_content_chunk:
                                repetition_count += 1
                                if repetition_count > 50: 
                                    log_debug("Repetition loop detected! Breaking.")
                                    break
                            else:
                                repetition_count = 0
                                last_content_chunk = refined_content
                                
                        # Check for tool call chunks (Thinking/Tool prep)
                        if msg.tool_call_chunks:
                            chunk = msg.tool_call_chunks[0]
                            
                            # Handle different chunk types safely
                            tool_name = None
                            if isinstance(chunk, dict):
                                tool_name = chunk.get("name")
                            elif hasattr(chunk, "name"):
                                tool_name = chunk.name
                                
                            if tool_name and tool_name != last_tool_call:
                                last_tool_call = tool_name
                                yield json.dumps({
                                    "type": "tool_start",
                                    "tool": last_tool_call,
                                    "args": {} 
                                }) + "\n"
                        
                        # Content Chunk (Real text response)
                        elif msg.content:
                            # Handle thinking blocks from various models
                            content = msg.content
                            
                            # Qwen3 uses <think>...</think>
                            # DeepSeek R1 also uses <think>...</think>
                            if '<think>' in content or '</think>' in content:
                                # Emit thinking event
                                thinking_content = content.replace('<think>', '').replace('</think>', '')
                                yield json.dumps({
                                    "type": "thinking", 
                                    "content": thinking_content 
                                }) + "\n"
                            else:
                                yield json.dumps({
                                    "type": "response", 
                                    "content": content 
                                }) + "\n"

                    # 2. Tool Output Message (When tool finishes)
                    elif isinstance(msg, ToolMessage):
                        yield json.dumps({
                            "type": "tool_output",
                            "output": msg.content
                        }) + "\n"
                        
            except Exception as e:
                log_debug(f"Stream Error: {e}")
                yield json.dumps({"type": "error", "content": str(e)}) + "\n"