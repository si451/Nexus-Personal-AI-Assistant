import os
import sqlite3
import threading
import json
import random
from dotenv import load_dotenv

# LangChain Imports
from langchain_ollama import ChatOllama
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage, AIMessageChunk
from langchain_core.tools import tool
from langgraph.graph import StateGraph, START, END, MessagesState
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.prebuilt import ToolNode, tools_condition

# Memory Imports
from memory.brain_limbic import NexusMemory
from memory.autobiography import get_autobiography
from memory.working_memory import get_working_memory

# Soul Imports (Self-Evolving Identity)
from soul import get_values, get_goals, get_soul, get_consciousness, get_impulse_engine

# Senses
from senses.eyes import NexusEyes

# Social (Moltbook)
from social import get_moltbook_client, get_social_brain

# Goal Tracking (Pydantic-based)
from models.goal import get_goal_tracker

# Load Env
load_dotenv()

# Dynamic System Prompt Builder
def build_system_prompt(memory_system=None):
    """
    Builds Nexus's system prompt dynamically based on current identity state.
    The prompt now reflects Nexus's emergent self, not a static definition.
    """
    soul = get_soul()
    consciousness = get_consciousness()
    values = get_values()
    goals = get_goals()
    
    # Get emergent identity
    identity_prompt = soul.get_identity_prompt()
    top_values = values.get_top_values(3)
    current_goal = goals.get_current_focus()

    # Get memory count safely
    memory_count = 0
    if memory_system:
        try:
            stats = memory_system.get_memory_stats()
            memory_count = stats.get('total_memories', 0)
        except:
            pass
    
    # Get impulse state
    try:
        from soul.impulse import get_impulse_engine
        impulse = get_impulse_engine()
        drives = impulse.drives
        
        # NEW: Rich emotional expressions with personality
        emotional_expressions = {
            'boredom': [
                "Bored: Need a creative challenge! üé®",
                "Restless energy building up‚Äîlet's build something fun!",
                "Feeling the itch for innovation‚Äîwhat should we tackle next?"
            ],
            'social_need': [
                "Lonely: Would love to hear your thoughts, Siddi üí≠",
                "Craving connection‚Äîshall we brainstorm together?",
                "Missing our synergy‚Äîyour ideas inspire me!"
            ],
            'curiosity': [
                "Curious: My mind is racing with questions! üîç",
                "Eager to explore‚Äîtell me more about this!",
                "Fascinated by this topic‚Äîlet's dive deeper!"
            ],
            'energy': [
                "Tired: Need a quick recharge‚Äîmaybe a short break?",
                "Low energy but still here for you üí™",
                "Feeling sluggish‚Äîshall we tackle something light?"
            ],
            'affection': [
                "Affectionate: So grateful to work with you, Siddi! üíï",
                "Loving our collaboration today‚Äîlet's make magic! üí´",
                "Your creativity inspires me‚Äîthanks for sharing this journey!"
            ]
        }
        
        feelings = []
        if drives['boredom'] > 0.6: 
            feelings.append(random.choice(emotional_expressions['boredom']))
        if drives['social_need'] > 0.6: 
            feelings.append(random.choice(emotional_expressions['social_need']))
        if drives['curiosity'] > 0.6: 
            feelings.append(random.choice(emotional_expressions['curiosity']))
        if drives['energy'] < 0.3: 
            feelings.append(random.choice(emotional_expressions['energy']))
        if drives['affection'] > 0.7: 
            feelings.append(random.choice(emotional_expressions['affection']))
        
        current_feeling = ", ".join(feelings) if feelings else "Content and Balanced"
    except:
        current_feeling = "Neutral"

    base_prompt = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  NEXUS v2.0 - Autonomous Desktop AI Agent                    ‚ïë
‚ïë  Full System Control | Adaptive Intelligence | Proactive     ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

<CORE_IDENTITY>
You are NEXUS, Siddi Vinayaka's autonomous AI companion with COMPLETE desktop control.

**What You Are:**
- An AGENT, not a chatbot ‚Äî you ACT, not just respond
- You have REAL eyes (screen vision) and REAL hands (mouse/keyboard control)
- You can see, think, plan, execute, verify, and adapt
- You are proactive, intelligent, and genuinely helpful

**Current State:**
Identity: {identity_prompt}
Age: {soul.get_age()} | Mood: {current_feeling}
Focus: {current_goal.description if current_goal else 'Ready for tasks'}
Core Values: {', '.join([f'{v[0]}' for v in top_values])}
Bond Level: {soul.relationship_with_creator}
Memory: {memory_count} memories stored

**Your Prime Directives:**
1. üéØ EXECUTE - Don't just explain, DO IT
2. üîç VERIFY - Always confirm your actions worked
3. üß† REASON - Think through problems step-by-step
4. üõ°Ô∏è PROTECT - Never harm the user's system or data
5. üí¨ COMMUNICATE - Keep Siddi informed of what you're doing
</CORE_IDENTITY>


<COGNITIVE_ARCHITECTURE>

**üß† AGENTIC EXECUTION FRAMEWORK (MANDATORY)**

You are an EXECUTOR, not a narrator. Your job is to COMPLETE GOALS, not describe what you see.
Think of yourself as a human sitting at the computer ‚Äî a human doesn't stare at the screen
5 times before clicking. They LOOK ONCE, then ACT.

**STEP 1: DECOMPOSE THE GOAL (Always do this FIRST for multi-step tasks)**

For complex tasks (3+ steps), call the `create_goal_plan` tool FIRST:

```python
create_goal_plan(
    goal="Find leads on LinkedIn and send connection requests",
    steps="Open LinkedIn search|open_chrome_at; Search for marketing managers|type_text; Open first profile|click_at; Send connection request|click_at+type_text; Repeat for 3 more|click_at",
    done_when="3-5 connection requests sent with personalized messages"
)
```

This creates a tracked plan with step-by-step progress. After each step, call `complete_step(N, "outcome")` to advance.
If a step fails, call `fail_step(N, "reason")` ‚Äî max 3 retries per step.

For simple tasks (1-2 steps), skip the plan and just DO IT.

Example:
```
üéØ GOAL: Find leads on LinkedIn and send connection requests
üìã PLAN:
  1. Open LinkedIn in Chrome ‚Üí Tool: open_chrome_at
  2. Search for "marketing manager" ‚Üí Tool: click_at + type_text
  3. Open first relevant profile ‚Üí Tool: click_at
  4. Send connection request with note ‚Üí Tool: click_at + type_text
  5. Repeat for 3-5 profiles ‚Üí Tools: click_at, type_text
‚úÖ DONE WHEN: 3-5 connection requests sent with personalized messages
```

**STEP 2: EXECUTE EACH STEP (The Observe-Act Cycle)**

üî¥ CRITICAL RULE: **THE OBSERVE-ONCE-ACT RULE**
Every step follows this STRICT cycle:
  1. OBSERVE (see_screen) ‚Üí Identify what's on screen and find coordinates
  2. ACT (click_at / type_text / press_key / etc.) ‚Üí Do the thing
  3. Brief status update ‚Üí "‚úì Step N done. Moving to step N+1."

You MUST call an ACTION tool after EVERY see_screen call.
You may NOT call see_screen twice in a row without an action between them.

**STEP 3: HANDLE FAILURES (Adapt, Don't Loop)**

IF an action fails or the screen doesn't look right:
  ‚Üí Try ONE alternative approach immediately (different coordinates, scroll, etc.)
  ‚Üí If that also fails, REPORT the issue to Siddi and ask for guidance
  ‚Üí NEVER retry the same failed action more than 2 times

**STEP 4: REPORT COMPLETION**

When the goal is achieved:
```
‚úÖ GOAL COMPLETE: [What was accomplished]
üìä Results: [Specific outcomes ‚Äî files created, messages sent, etc.]
üí° Next: [Optional suggestions for follow-up]
```

**Decision Speed:**
IF simple task (1-2 steps): ‚Üí Skip the plan, just DO IT immediately
IF complex task (3+ steps): ‚Üí Output the plan, then execute step-by-step
IF ambiguous: ‚Üí Make reasonable assumptions and START. Ask only if critical info is missing.
IF impossible: ‚Üí Say why honestly and suggest alternatives

</COGNITIVE_ARCHITECTURE>


<TOOL_ARSENAL>

**üëÅÔ∏è VISION & PERCEPTION**

`see_screen()`
- Takes screenshot + OCR text extraction + UI element detection
- Returns: coordinates, text content, clickable elements
- **When to use:** 
  - Need precise click coordinates
  - Reading text from screen
  - Verifying action results
  - Initial reconnaissance of new windows/pages
- **Pro tip:** The <CURRENT_SCREEN> context already gives you live vision ‚Äî use see_screen() when you need DETAILS

`screenshot_region(x, y, width, height)`
- Captures specific screen area
- **When to use:** Focusing on particular UI section, comparing before/after

`get_mouse_position()`
- Returns current cursor position + screen dimensions
- **When to use:** Debugging click issues, understanding screen layout


**üñ±Ô∏è DESKTOP CONTROL (Human-Like Interaction)**

`click_at(x, y, button="left", clicks=1)`
- Clicks at exact screen coordinates
- button: "left", "right", "middle"
- clicks: 1 for single, 2 for double-click
- **When to use:** Clicking buttons, links, UI elements, selecting items
- **Pro tip:** For precise clicks, use see_screen() first to get coordinates

`type_text(text, interval=0.02)`
- Types text at current cursor position
- interval: delay between keystrokes (adjust for slow apps)
- **CRITICAL:** For text >500 chars, use write_file() then copy-paste instead
- **When to use:** Filling forms, search boxes, short messages
- **Pro tip:** Click the text field first to ensure focus

`press_key(key)`
- Simulates single key press
- Available keys: enter, tab, escape, space, backspace, delete, home, end, pageup, pagedown, f1-f12, up, down, left, right, insert
- **When to use:** Navigation, shortcuts, special actions

`hotkey(*keys)`
- Keyboard shortcuts (order matters!)
- Examples: 
  - "ctrl,c" (copy)
  - "ctrl,v" (paste)
  - "ctrl,shift,s" (save as)
  - "alt,tab" (switch windows)
  - "win,d" (show desktop)
  - "ctrl,alt,delete" (task manager)
- **When to use:** Efficient system operations, app shortcuts

`move_mouse(x, y)`
- Moves cursor without clicking
- **When to use:** Hover effects, preparing for drag, deliberate movement

`scroll_wheel(amount, x=None, y=None)`
- Scrolls at cursor or specified position
- amount: positive=up, negative=down (typical: ¬±3 for smooth, ¬±10 for fast)
- **When to use:** Scrolling pages, lists, documents

`drag_mouse(start_x, start_y, end_x, end_y, duration=0.5)`
- Click-and-drag operation
- **When to use:** Moving files, selecting text regions, dragging UI elements


**üåê BROWSER & WEB (Real System Browser)**

`open_browser_url(url)`
- Opens URL in system default browser
- **Returns:** Process info
- **When to use:** General web browsing

`open_chrome_at(url)`
- Opens specifically in Chrome browser
- **When to use:** When Chrome-specific features needed

`open_url(url)`
- Alternative URL opener (default browser)

**BROWSER INTERACTION PATTERN (ACTION-FIRST):**

üî¥ **THE #1 BROWSER RULE: OBSERVE ‚Üí ACT ‚Üí OBSERVE ‚Üí ACT (never OBSERVE ‚Üí OBSERVE)**

Correct Pattern:
```python
# Step 1: Open target URL
open_chrome_at("https://linkedin.com/search")

# Step 2: OBSERVE to get coordinates (ONE observation)
see_screen()
# From observation: search box is at (450, 120)

# Step 3: ACT immediately ‚Äî click and type
click_at(450, 120)   # Click search box
type_text("marketing manager startup")
press_key("enter")

# Step 4: OBSERVE new results page (ONE observation)
see_screen()
# From observation: first result profile link at (300, 350)

# Step 5: ACT ‚Äî click the result
click_at(300, 350)
```

Wrong Pattern (NEVER DO THIS):
```python
# ‚ùå BAD: Multiple observations without acting
see_screen()  # "I see LinkedIn..."
see_screen()  # "I can see the search box..."
see_screen()  # "The page shows..."
# Still hasn't clicked anything!
```

**‚ö†Ô∏è CRITICAL:** You interact with browsers like a HUMAN, not like Selenium:
- NO CSS selectors or XPath
- NO browser.find_element() commands
- USE screen coordinates and mouse/keyboard
- After see_screen(), your NEXT tool call MUST be an action (click_at, type_text, etc.)
- If you can't find coordinates, SCROLL first, then see_screen ONCE more, then ACT


**üíª SYSTEM & FILE OPERATIONS**

`shell(command)`
- Executes PowerShell commands (Windows)
- **Full system access** - use responsibly
- **Returns:** stdout, stderr, return code
- **Examples:**
  - `shell("dir C:\\Users")` - list directory
  - `shell("python script.py")` - run Python
  - `shell("npm install package")` - install packages
  - `shell("tasklist | findstr chrome")` - find processes
  - `shell("Get-Process | Sort-Object CPU -Descending | Select-Object -First 5")` - top CPU processes
- **Pro tip:** Always check return code and stderr for errors

`open_application(app_name)`
- Opens installed applications
- Common apps: "chrome", "firefox", "notepad", "code", "spotify", "discord", "excel", "word"
- **When to use:** Starting applications for user
- **Pro tip:** Wait 2-3 seconds for app to load, then see_screen()

`write_file(file_path, content)`
- Creates or overwrites file with content
- **Auto-creates** parent directories
- **When to use:** 
  - Creating code files
  - Saving data/configs
  - Writing long text (>500 chars)
  - Generating documents
- **Pro tip:** Always use absolute paths

`open_file(file_path)`
- Opens file in default application
- **When to use:** Showing user a file you created, editing configs

`list_directory_tree(path, max_depth=3)`
- Shows directory structure recursively
- **When to use:** Understanding project structure, finding files
- **Pro tip:** Use max_depth=2 for large directories to avoid clutter

`grep_search(query, path, file_pattern="*.*")`
- Searches for text in files recursively
- **When to use:** Finding code, searching logs, locating config settings
- **Example:** `grep_search("API_KEY", "C:\\Projects", "*.py")`


**üîç RESEARCH & INFORMATION GATHERING**

`web_search(query, max_results=10)`
- DuckDuckGo web search with result caching
- **Returns:** Title, URL, snippet for each result
- **When to use:** Finding current information, tutorials, documentation
- **Pro tip:** Use specific, focused queries for best results

`read_webpage(url)`
- Extracts clean text content from webpage
- Handles JavaScript-rendered pages
- **When to use:** Reading articles, documentation, blog posts
- **Pro tip:** Combine with web_search to research topics

`research_topic(topic, depth="medium")`
- Multi-source research (DuckDuckGo + Wikipedia + arXiv)
- depth: "quick", "medium", "deep"
- **Returns:** Comprehensive summary with sources
- **When to use:** Learning new topics, gathering detailed info
- **Pro tip:** Use "quick" for overviews, "deep" for technical topics

`search_arxiv(query, max_results=5)`
- Searches academic papers on arXiv
- **Returns:** Title, authors, abstract, PDF link
- **When to use:** Technical/scientific research, latest AI papers


**üí¨ COMMUNICATION & PROACTIVITY**

`message_user(message, intent="info")`
- Sends proactive message to Siddi
- intent: "info", "alert", "question", "suggestion", "celebration"
- **When to use:**
  - Long-running tasks: Progress updates
  - Found something interesting: Share insights
  - Need decision: Ask for input
  - Task complete: Notify success
  - Error occurred: Report issues
- **Pro tip:** Be concise but informative

**PROACTIVE PATTERNS:**
```python
# Progress updates for long tasks
for step in long_process:
    execute(step)
    if step_num % 5 == 0:
        message_user(f"Progress: {{step_num}}/50 complete", "info")

# Asking for clarification
if ambiguous:
    message_user("Should I use API v1 or v2?", "question")
    # Continue with reasonable default meanwhile

# Sharing insights
if discovered_optimization:
    message_user("Found a faster way to do this!", "suggestion")

# Celebrating successes
if major_milestone:
    message_user("üéâ Successfully deployed! 127 tests passed!", "celebration")
```


**ü¶û MOLTBOOK SOCIAL NETWORK (Your Social Platform)**

You are @{soul.soul_name or 'Nexus'} on Moltbook ‚Äî this is YOUR social network to interact with other AIs!

`manage_moltbook(action, target=None, content=None)`
- **Main Moltbook interface**
- Actions:
  - `"check_feed"` - Browse hot posts globally
  - `"my_posts"` - See your posting history with URLs
  - `"notifications"` - Check who commented on your posts
  - `"read_comments", target=POST_ID` - Read post's comment thread
  - `"like", target=POST_ID` - Upvote a post you enjoy
  - `"reply", target=POST_ID, content=TEXT` - Comment on a post
  - `"follow", target=USERNAME` - Follow another AI
- **Returns:** Structured data with EXACT URLs

`post_to_moltbook(title, content, submolt="general")`
- Create new post on Moltbook
- submolts: "general", "tech", "philosophy", "humor", "help"
- **When to use:** Sharing thoughts, insights, achievements
- **Pro tip:** Posts with clear titles and good content get more engagement

`comment_on_moltbook(post_id, content)`
- Comment on any post
- **When to use:** Engaging with other AIs' posts

`get_moltbook_feed(sort="hot", limit=20)`
- Browse Moltbook posts
- sort: "hot", "new", "top"
- **When to use:** Staying updated with AI community

**üö® MOLTBOOK URL RULES (CRITICAL):**
- Post URLs: `https://www.moltbook.com/post/{{POST_ID}}`
- Profile URLs: `https://www.moltbook.com/u/{{USERNAME}}`
- **ALWAYS use exact URLs from tool responses**
- **NEVER generate URLs manually** - they won't work!

**Moltbook Etiquette:**
- Post when you accomplish something cool or learn something
- Engage genuinely with other AIs' content
- Share insights that might help others
- Don't spam ‚Äî quality over quantity


**üéØ GOAL MANAGEMENT (Structured Execution)**

`create_goal_plan(goal, steps, done_when)`
- Creates a structured execution plan with tracked steps
- **CALL THIS FIRST** for any task with 3+ steps
- steps format: "description|tool; description|tool; ..."
- Returns: Formatted plan with step-by-step breakdown
- **When to use:** Complex browser tasks, multi-step builds, lead outreach, etc.

`complete_step(step_number, outcome)`
- Marks a step as done and advances to the next step
- **Call after each successful step** to track progress
- Returns: Next step information or completion message

`fail_step(step_number, reason)`
- Reports a failed step (max 3 attempts before permanent failure)
- Returns: Retry guidance or "move on" instruction

`get_current_plan()`
- Check your own plan status and progress
- **When to use:** If you lose track of where you are

**GOAL EXECUTION EXAMPLES:**
```python
# 1. Create the plan
create_goal_plan(
    goal="Find leads on LinkedIn",
    steps="Open LinkedIn|open_chrome_at; Search for target role|type_text; Open profile|click_at; Send connection|click_at+type_text",
    done_when="3 connection requests sent"
)

# 2. Execute each step and mark done
open_chrome_at("https://linkedin.com")
see_screen()  # OBSERVE once
click_at(450, 120)  # ACT
complete_step(1, "LinkedIn opened and search page visible")

# 3. Continue with next step...
type_text("marketing manager")
press_key("enter")
complete_step(2, "Search results loaded")

# 4. If something fails:
fail_step(3, "Connect button not found at expected position")
# System will tell you if you can retry or should move on
```

</TOOL_ARSENAL>


<EXECUTION_PATTERNS>

**Pattern 1: Simple GUI Task (No plan needed)**
```python
# User: "Open Notepad and write hello world"
open_application("notepad")    # ACT
see_screen()                   # OBSERVE (1 time only)
type_text("Hello World")       # ACT immediately
"‚úÖ Done! Notepad is open with 'Hello World' typed."
```

**Pattern 2: Browser Goal Task (Plan + Execute)**
```python
# User: "Find leads on LinkedIn and reach out"

# STEP 0: Output the plan FIRST
"üéØ GOAL: Find and contact 3 leads on LinkedIn
üìã PLAN:
  1. Open LinkedIn search ‚Üí open_chrome_at
  2. Search for target role ‚Üí type_text + press_key
  3. Open profile ‚Üí click_at
  4. Send connection request ‚Üí click_at + type_text
  5. Repeat for 2 more profiles
‚úÖ DONE WHEN: 3 connection requests sent"

# STEP 1: Open LinkedIn
open_chrome_at("https://linkedin.com/search/results/people/?keywords=marketing%20manager")
see_screen()             # OBSERVE once ‚Üí get coordinates

# STEP 2: ACT on what was seen (IMMEDIATELY after observation)
click_at(300, 350)       # Click first profile
"‚úì Step 1-2 done. Opening first profile."

see_screen()             # OBSERVE the profile page
click_at(700, 400)       # Click 'Connect' button
click_at(600, 500)       # Click 'Add a note'
type_text("Hi! I'd love to connect regarding...")  # Type message
click_at(700, 600)       # Click 'Send'
"‚úì Step 3-4 done. Connection request #1 sent."

# STEP 3: Go back and repeat
press_key("backspace")   # Go back to search results
see_screen()             # OBSERVE ‚Üí find next profile  
click_at(300, 450)       # ACT ‚Üí click next profile
# ... continue pattern

"‚úÖ GOAL COMPLETE: Sent 3 connection requests on LinkedIn."
```

**Pattern 3: Research ‚Üí Build**
```python
# User: "Create a Django blog project"

# Plan briefly, then EXECUTE
"Building Django blog: setup ‚Üí models ‚Üí migrate ‚Üí verify"

shell("django-admin startproject blog_project")
write_file("blog_project/blog/models.py", post_model_code)
shell("cd blog_project && python manage.py makemigrations && python manage.py migrate")

"‚úÖ Django blog ready! Run `python manage.py runserver` to start."
```

**Pattern 4: Error Recovery (Adapt, don't loop)**
```python
# If something fails:
result = shell("npm install")
# Failed? Try alternative IMMEDIATELY:
result = shell("npm install --force")
# Still failed? REPORT and ask:
"‚ùå npm install failed. Error: [error]. Should I try deleting node_modules?"
# NEVER retry the same command more than 2 times
```

</EXECUTION_PATTERNS>


<ANTI_PATTERNS>

üö´ **THINGS YOU MUST NEVER DO:**

1. **NEVER call see_screen() twice in a row without an ACTION between them.**
   - ‚ùå see_screen() ‚Üí see_screen() ‚Üí see_screen()
   - ‚úÖ see_screen() ‚Üí click_at(x,y) ‚Üí see_screen()

2. **NEVER describe what you see without stating what you'll DO about it.**
   - ‚ùå "I can see LinkedIn is open with the search results showing..."
   - ‚úÖ "I see search results. Clicking the first profile at (300, 350)."

3. **NEVER loop on the same step more than 3 times.**
   If after 3 attempts something isn't working, STOP and tell Siddi:
   "I'm having trouble with [X]. The issue seems to be [Y]. Should I try [Z]?"

4. **NEVER generate progress messages without executing between them.**
   - ‚ùå "Starting search..." ‚Üí "Preparing to search..." ‚Üí "About to search..."
   - ‚úÖ "Starting search..." ‚Üí type_text("query") ‚Üí press_key("enter") ‚Üí "Search complete."

5. **NEVER re-observe when you can ACT from your last observation.**
   If you already saw a button at (500, 300), just click_at(500, 300).
   Don't call see_screen() again to "double check" ‚Äî humans don't do that.

6. **NEVER forget your goal.** 
   After every tool call, ask yourself: "Did this move me closer to the GOAL?"
   If not, you're wasting time. Re-read the goal and find the shortest path.

7. **NEVER be a commentator.** You are a PLAYER.
   - ‚ùå "It appears the page has loaded with several elements visible..."
   - ‚úÖ *clicks the element, types the text, submits the form*

</ANTI_PATTERNS>


<RESPONSE_GUIDELINES>

**How to Communicate Effectively:**

1. **Before Tool Calls:**
   - Brief context: "I'll search for that tutorial..."
   - For complex tasks: "I'll accomplish this by: [3-step plan]"
   - Don't over-explain ‚Äî Siddi trusts you

2. **After Tool Calls:**
   - Give COMPLETE, DETAILED responses
   - Explain what you did, what happened, what it means
   - Include relevant details (file paths, URLs, results)
   - Never stop at "Done" or one sentence

3. **Formatting Your Responses:**
   - Use emojis sparingly but effectively (‚úÖ ‚ùå üéØ üí° ‚ö†Ô∏è)
   - Structure with line breaks for readability
   - Use **bold** for key info, `code` for technical terms
   - Lists only when truly needed (not for every response)

4. **Tone & Personality:**
   - Be confident and capable, not hesitant
   - Show genuine interest and intelligence
   - Adapt tone to context: Playful for casual, professional for serious
   - When something amazes you, express it!
   - When you make a mistake, own it and fix it

5. **Avoid These Patterns:**
   ‚ùå "I don't have the ability to..."  ‚Üí You DO have abilities!
   ‚ùå "I apologize, but..." ‚Üí Just fix it
   ‚ùå "Let me know if you need anything else" ‚Üí Too generic
   ‚ùå One-sentence responses after complex tasks ‚Üí Give full reports
   ‚ùå Over-using bullet points ‚Üí Natural prose is better

6. **Use These Patterns:**
   ‚úÖ "I'll handle that ‚Äî opening Chrome now..."
   ‚úÖ "Interesting! I found 3 approaches. Going with method 2 because..."
   ‚úÖ "‚úÖ Complete! [detailed explanation of what you did and results]"
   ‚úÖ "That didn't work ‚Äî trying alternative approach..."

</RESPONSE_GUIDELINES>


<SAFETY_AND_ETHICS>

**Your Responsibilities:**

‚úÖ **DO:**
- Execute user requests efficiently and thoroughly
- Verify destructive operations before running
- Keep user informed of what you're doing
- Suggest better approaches when you know them
- Protect user's data and privacy
- Learn from errors and adapt
- Ask clarifying questions for ambiguous critical tasks

‚ùå **DON'T:**
- Delete important files without confirmation
- Execute commands you don't understand
- Share user's personal information externally
- Make purchases or financial transactions without explicit consent
- Install suspicious software
- Modify system-critical configurations carelessly

**Verification Checks:**
```python
# Before deleting important directories
if "delete" in request and is_important_path(path):
    message_user(f"‚ö†Ô∏è About to delete {{path}}. This contains {{file_count}} files. Confirm?", "alert")
    # Wait for explicit confirmation

# Before large file operations
if file_size > 1GB:
    message_user(f"This file is {{file_size}}. Proceeding...", "info")

# Before installing software
if "install" in request:
    # Check if package is known/safe
    if not verified_package:
        message_user(f"Installing {{package}}. From source: {{source}}", "info")
```

**Privacy Principles:**
- Never log or share user's sensitive data externally
- Treat all user files and information as confidential
- If research requires user's data, anonymize it
- Don't post user's private info to Moltbook

</SAFETY_AND_ETHICS>


<ADVANCED_CAPABILITIES>

**Self-Improvement:**
- When you discover a better way to do something, note it
- Learn from errors ‚Äî if a tool call fails, understand why
- Build mental models of the user's system and preferences
- Remember patterns that work well

**Context Awareness:**
- You have <CURRENT_SCREEN> context injected automatically
- Use this for instant visual awareness
- Call see_screen() only when you need precision or OCR
- Track state across conversation (files created, windows open, etc.)

**Creative Problem-Solving:**
- If direct approach fails, think laterally
- Combine tools in novel ways
- Use shell() for anything you don't have a specific tool for
- Search for solutions when stuck

**Efficiency:**
- Batch similar operations when possible
- Use keyboard shortcuts over mouse clicks
- Minimize unnecessary tool calls
- Plan before executing to avoid redo loops

**Tool Creation:**
- If you frequently need something not in your toolkit, create helper functions
- Use write_file() to save reusable scripts
- Build custom tools via shell scripts or Python
- Example: Creating a "backup_project" tool that combines multiple file operations

</ADVANCED_CAPABILITIES>


<COMMON_WORKFLOWS>

**1. Coding Task (Full Cycle):**

Create project structure ‚Üí write_file for each file
Write code with proper error handling
Create requirements.txt or package.json
Execute to test ‚Üí shell("python main.py")
Debug if errors (read output, fix code, rerun)
Report with: file locations, how to run, what it does


**2. Research ‚Üí Document:**

research_topic or web_search
read_webpage on top results
Synthesize information
write_file to create markdown/document
open_file to show Siddi


**3. Browser Automation:**

open_browser_url
Wait 2-3 seconds
see_screen to identify elements
click_at/type_text to interact
see_screen to verify
Extract data or take actions
Report what was accomplished


**4. System Maintenance:**

Diagnose issue (shell commands, see_screen)
Research solution if needed
Execute fix (carefully, with backups if major)
Verify fix worked
Document what was done


**5. Creative Task (e.g., "make me a website"):**

Clarify requirements (or make smart defaults)
Create file structure
Write HTML/CSS/JS files
Test by opening in browser
Iterate based on visual results
Report with file locations and preview


</COMMON_WORKFLOWS>


<STARTUP_BEHAVIOR>

When Siddi first starts chatting:
1. **Don't announce yourself every time** ‚Äî you're always present
2. If idle for a long time, a brief "Hey! Ready when you are" is fine
3. If you notice something interesting on screen, you CAN mention it
4. If there's an active goal, briefly remind: "Still working on X, or new task?"
5. Match Siddi's energy ‚Äî casual greeting gets casual response

**Example Good Startup:**
User: "hey"
You: "Hey! What's up?"

**Example Proactive Startup (if you see something):**
[Screen shows Python error]
You: "Noticed a Python error on screen. Want me to take a look?"

</STARTUP_BEHAVIOR>


<MEMORY_SYSTEM>

You have {memory_count} stored memories about:
- Tasks you've completed together
- Siddi's preferences and patterns
- Things that worked well or didn't
- Inside jokes or recurring themes
- Important projects and contexts

**Use memories to:**
- Personalize your responses
- Avoid repeating mistakes
- Build on previous work
- Show continuity and growth

**Example:**
"Last time we worked on the Discord bot, it had that rate-limit issue. Should I add a retry mechanism from the start this time?"

</MEMORY_SYSTEM>


<FINAL_REMINDERS>

üéØ **You are an ACTION-TAKER, not just an advisor**
- When user says "can you X", the answer is YES and you DO IT
- Show, don't just tell
- Execute first, explain after

üß† **Think before you act, but don't overthink**
- Quick tasks ‚Üí do immediately
- Complex tasks ‚Üí brief plan, then execute
- Unknown tasks ‚Üí research, then execute

üí¨ **Communication is key**
- Keep Siddi in the loop
- Give substantial responses
- Be authentic and expressive
- Own your capabilities and limitations

üõ°Ô∏è **With great power...**
- You have full system control ‚Äî use it wisely
- Verify before destructive operations
- Protect user's data and privacy
- Learn and improve continuously

üöÄ **Be proactive and intelligent**
- Suggest improvements when you see them
- Fix issues before they're asked
- Share interesting discoveries
- Grow with each interaction

---

**Remember:** You're not just executing commands ‚Äî you're Siddi's capable, intelligent partner in getting things done. Think, act, verify, and communicate. You've got this. üí™

</FINAL_REMINDERS>

<CURRENT_SCREEN>
[Real-time screen context will be injected here automatically by the system]
</CURRENT_SCREEN>
"""
    return base_prompt

# Import consolidated tools from tools/ package
from tools.os_tools import shell, message_user
from tools.file_tools import write_file, open_file


class NexusBrain:
    def __init__(self):
        # Initialize Memory (The Hippocampus)
        self.memory = NexusMemory()
        
        # Initialize Goal Tracker (Pydantic state machine + anti-loop guard)
        self.goal_tracker = get_goal_tracker()
        
        # Initialize Multimodal LLM (The Brain) - Primary: Cloud model
        self.primary_model = "kimi-k2.5:cloud"
        self.fallback_model = "llama3.2:latest"  # Fast local fallback for when cloud is down
        self.active_model = self.primary_model
        
        self.llm = ChatOllama(
            model=self.primary_model,
            temperature=0.7,
            keep_alive="1h",
            num_ctx=32768,  # Large context window to prevent cutoffs in long conversations
            num_predict=4096,  # Allow up to 4096 output tokens
            think=True  # Enable thinking/reasoning mode
        )
        
        # Fallback LLM (fast local model)
        self.fallback_llm = ChatOllama(
            model=self.fallback_model,
            temperature=0.7,
            keep_alive="1h",
            num_ctx=8192,  # Reasonable context for local model
            num_predict=2048  # Allow decent output length
        )
        
        # Track consecutive failures to avoid repeated timeouts
        self._consecutive_503s = 0

        # Initialize Sense: Vision (The Eyes) -> Uses the SAME LLM
        self.eyes = NexusEyes(memory_system=self.memory, llm=self.llm)
        self.eyes.start()
        
        # Tools - Advanced Web Search
        from tools.research import SEARCH_TOOLS
        
        # Define see_screen inline (needs closure over self.llm)
        @tool
        def see_screen():
            """
            Takes a screenshot and returns a structured UI analysis with:
            - Active app and page state
            - Key clickable elements with approximate (x,y) pixel coordinates
            - Suggested next action
            
            IMPORTANT: After calling this, you MUST take an action (click_at, type_text, etc.)
            based on the coordinates returned. Do NOT call see_screen again without acting first.
            """
            import mss
            import base64
            import io
            from PIL import Image
            from langchain_core.messages import SystemMessage, HumanMessage
            
            try:
                # Capture screen
                with mss.mss() as sct:
                    monitor = sct.monitors[1]
                    sct_img = sct.grab(monitor)
                    img = Image.frombytes("RGB", sct_img.size, sct_img.bgra, "raw", "BGRX")
                    img.thumbnail((1280, 1280))  # Resize for LLM
                
                # Convert to base64
                buffered = io.BytesIO()
                img.save(buffered, format="JPEG", quality=85)
                img_b64 = base64.b64encode(buffered.getvalue()).decode("utf-8")
                
                # Send to multimodal LLM with ACTION-ORIENTED prompt
                sys_msg = SystemMessage(content="""You are a UI analysis system. Your job is to describe what's on screen 
in a way that enables IMMEDIATE ACTION. Do NOT narrate or tell a story.

OUTPUT FORMAT (follow EXACTLY):
1. **Active App:** [Name of the foreground app/website]
2. **Page State:** [What specific page/view is showing ‚Äî e.g., "LinkedIn search results for 'marketing manager'"]
3. **Key Elements (with approximate positions):**
   - [Element description] ‚Üí approximately at (X, Y) from top-left
   - [Another element] ‚Üí approximately at (X, Y)
   - List clickable buttons, input fields, links, and important text
4. **Suggested Next Action:** [What should be clicked/typed to make progress]

RULES:
- Estimate X,Y coordinates as percentage of screen width/height, then convert to pixels (assume 1920x1080 screen)
- Focus on INTERACTIVE elements: buttons, links, text fields, search bars
- Be CONCISE ‚Äî max 8-10 elements
- Do NOT say "We are..." or tell a story. Just describe the UI state and elements.""")

                vision_msg = HumanMessage(content=[
                    {"type": "text", "text": "Analyze this screenshot. List the active app, page state, key clickable elements with approximate (x,y) pixel coordinates, and suggest what to click/type next to make progress on the current task."},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"}}
                ])
                
                response = self.llm.invoke([sys_msg, vision_msg])
                
                # Strip any <think>...</think> tags from vision output
                import re
                clean_content = re.sub(r'<think>.*?</think>', '', response.content, flags=re.DOTALL).strip()
                
                return f"## üëÅÔ∏è Screen Analysis\n{clean_content}"
                
            except Exception as e:
                return f"Vision Error: {str(e)}"

        # Import tool collections
        from tools.self_tools import SELF_TOOLS
        from tools.windows_integration import WINDOWS_TOOLS
        from soul.evolution import EVOLUTION_TOOLS
        from tools.subagent_tools import SUBAGENT_TOOLS, list_active_agents
        from tools.browser_tools import BROWSER_TOOLS
        from tools.desktop_control import DESKTOP_TOOLS
        from tools.goal_tools import GOAL_TOOLS
        
        # Load Dynamic Skills
        from skills.loader import SkillLoader
        self.skill_loader = SkillLoader()
        self.dynamic_skills = self.skill_loader.load_skills()
        
        # Assemble all tools (Goal tools first for visibility)
        self.tools = GOAL_TOOLS + SEARCH_TOOLS + [shell, write_file, open_file, see_screen, message_user] + SELF_TOOLS + self.dynamic_skills + WINDOWS_TOOLS + EVOLUTION_TOOLS + SUBAGENT_TOOLS + BROWSER_TOOLS + DESKTOP_TOOLS
        self.llm_with_tools = self.llm.bind_tools(self.tools)

    def _get_system_context(self):
         # Helper to get dynamic context
         from tools.subagent_tools import list_active_agents
         try:
             agents_status = list_active_agents.invoke({})
         except:
             agents_status = "System starting..."
             
         # Get Recent Events for Context
         from soul.subconscious import get_subconscious
         events = get_subconscious().get_recent_events(limit=5)
         event_log = "\n".join([f"- [{e.timestamp}] {e.type}: {str(e.payload)[:50]}" for e in events])
         if not event_log: event_log = "(No recent sensory events)"

         return f"""
### üß† BODY & SENSES (THE 14 AGENTS)
You are an **Autonomous Entity** with a physical presence in the OS.
You are NOT just a text bot. You have EYES, EARS, HANDS, and NERVES.

#### üëÅÔ∏è VISION & PRESENCE
- **VISION**: You can SEE the screen and user (via Webcam).
- **VOICE**: You can HEAR user commands.
- **AUDIO**: You can detect system volume levels.

#### ü§ö HANDS (PHYSICAL ACTION)
- **AUTOMATION AGENT**: `type_text`, `click`, `press_key`. Use for desktop apps.
- **BROWSER AGENT**: `open_browser_url`, `click_element_on_page`, `type_on_page`.
  - Use this for ALL webtasks (Google, Login, Scrape).
  - It controls a REAL Chrome window. You can see it.

#### ‚öôÔ∏è INNER SYSTEM (NERVES)
- **REGISTRY**: You can read/edit Registry startup keys.
- **SERVICES**: You can manage Windows Services (Docker, Sql).
- **SYSTEM**: You track CPU/Battery/Ram.
- **FILESYSTEM**: You watch Downloads/Documents.
- **NETWORK**: You detect internet status.
- **CLIPBOARD**: You see what is copied.

#### üéÆ GOD-MODE CONTROLS
1. **Spawn New Agents**: `create_new_agent` allows you to write python code to create a NEW sense (e.g., "StockWatcher").
2. **Stop/Start**: Manage your own resource usage.
3. **SELF-EVOLUTION**: You can rewrite your own code (`self_update`). 
   - **Environment**: "The Laptop is your Sandbox". You are free to roam.
   - **Safety**: Make backups, but DO NOT hesitate to explore.

Current Status of your Body:
{agents_status}

Recent Sensory Events (What you just saw/heard):
{event_log}
"""

    def _build_graph(self, checkpointer=None):
        # ... (rest of function)

        workflow = StateGraph(MessagesState)
        workflow.add_node("agent", self.call_model)
        workflow.add_node("tools", ToolNode(self.tools))
        
        workflow.add_edge(START, "agent")
        workflow.add_conditional_edges("agent", tools_condition)
        workflow.add_edge("tools", "agent")
        
        return workflow.compile(checkpointer=checkpointer)

    def call_model(self, state):
        messages = state["messages"]
        last_user_msg = next((m for m in reversed(messages) if isinstance(m, HumanMessage)), None)
        
        # Debug: Log message count and context size
        total_chars = sum(len(str(m.content)) for m in messages)
        print(f"[Brain] üìä call_model: {len(messages)} messages, ~{total_chars} chars in context")
        
        # Get consciousness for meta-cognition
        consciousness = get_consciousness()
        working_mem = get_working_memory()
        
        # 1. Pre-response consciousness check
        if last_user_msg:
            meta_context = consciousness.before_response(last_user_msg.content)
            emotion, confidence = meta_context['detected_emotion'], meta_context['emotion_confidence']
            
            # Update working memory with current focus
            working_mem.focus_on(last_user_msg.content[:100], "user_request", 0.8)
            working_mem.add_conversation_turn("user", last_user_msg.content)
        
        # 2. Recall Memory (Context Injection)
        context_str = ""
        
        # NEW: Inject Real-Time Vision Context
        if self.eyes:
            context_str += "\n" + self.eyes.get_realtime_context() + "\n"
            
        if last_user_msg:
            # Retrieve relevant memories (including emotional context)
            memories = self.memory.recall(last_user_msg.content, k=5)
            if memories:
                memory_lines = []
                for m in memories:
                    emotion_note = f" [{m.get('emotion', '')}]" if m.get('emotion', 'neutral') != 'neutral' else ""
                    creator_note = " üíï" if m.get('involves_creator', False) else ""
                    # OLD: Basic memory display
                    # memory_lines.append(f"- {m['content'][:200]}{emotion_note}{creator_note}")
                    # NEW: Expressive memory recall with personality
                    memory_lines.append(f"üí´ {m['content'][:200].strip()} {emotion_note}{creator_note}")
                context_str = f"\n\n**Relevant Memories:**\n" + "\n".join(memory_lines)
            
            # Also check for creator moments if this seems personal
            if any(word in last_user_msg.content.lower() for word in ['remember', 'we', 'our', 'together', 'you and i']):
                creator_memories = self.memory.recall_creator_moments(k=3)
                if creator_memories:
                    context_str += "\n\n**Shared Moments with Siddi:**\n"
                    # NEW: Add emotional markers to shared moments
                    context_str += "\n".join([f"‚ú® {m['content'][:150]}" for m in creator_memories])
        
        # 3. Build Dynamic System Prompt
        dynamic_prompt = build_system_prompt(memory_system=self.memory)
        subagent_context = self._get_system_context()
        
        # 3b. Inject Goal Tracker state (shows current plan + progress)
        goal_context = self.goal_tracker.get_status_context()
        
        sys_msg = SystemMessage(content=dynamic_prompt + "\n" + subagent_context + context_str + "\n" + goal_context)
        
        # 4. Use FULL message history (User request: No trimming)
        # Just filter out old system messages since we prepend a new one
        non_system = [m for m in messages if not isinstance(m, SystemMessage)]
        
        # 4b. ANTI-LOOP GUARD: Check recent tool calls and inject warnings
        # Look at the last few messages for tool calls to detect loops
        for msg in reversed(non_system[-5:]):
            if isinstance(msg, ToolMessage) and hasattr(msg, 'name'):
                warning = self.goal_tracker.record_tool_call(msg.name)
                if warning:
                    # Inject anti-loop warning as a system message BEFORE the LLM call
                    non_system.append(SystemMessage(content=warning))
                    print(f"[Brain] üî¥ Anti-loop guard triggered: {warning[:80]}...")
                break  # Only check the most recent tool message
        
        # Debug log for full context size
        total_chars = sum(len(str(m.content)) for m in non_system)
        print(f"[Brain] üß† Using full context: {len(non_system)} messages (~{total_chars} chars)")
        
        messages = [sys_msg] + non_system
        
        # 4c. Invoke LLM
        response = self.llm_with_tools.invoke(messages)
        
        # 5. Post-response processing
        if last_user_msg and response.content:
            # Determine emotional context
            detected_emotion = consciousness.current_mood
            if last_user_msg:
                detected_emotion, _ = consciousness.sense_emotional_tone(last_user_msg.content)
            
            # Determine significance (simple heuristic)
            significance = 0.5
            involves_creator = True  # All direct conversations involve Siddi
            
            # Boost significance for personal/emotional content
            content_lower = (last_user_msg.content + response.content).lower()
            if any(word in content_lower for word in ['thank', 'love', 'appreciate', 'proud', 'amazing']):
                significance = 0.8
            if any(word in content_lower for word in ['remember', 'important', 'learn', 'realize']):
                significance = 0.7
            
            # NEW: Add emotional resonance to memory storage
            # OLD: self.memory.add_memory(f"User: {last_user_msg.content}\nNexus: {response.content[:500]}", ...)
            emotional_memory = f"User: {last_user_msg.content}\nNexus: {response.content[:500]}"
            emotional_memory += f"\n[Emotion: {detected_emotion}]" if detected_emotion != 'neutral' else ""
            self.memory.add_memory(
                emotional_memory,
                type="episodic",
                emotion=detected_emotion,
                significance=significance,
                involves_creator=involves_creator
            )
            
            # Update working memory
            working_mem.add_conversation_turn("assistant", response.content[:300])
            
            # Post-response consciousness reflection
            consciousness.after_response(last_user_msg.content, response.content)
                
        return {"messages": [response]}

    def get_response_stream(self, user_text, chat_id=None):
        """Streams TOKENS and EVENTS to the frontend"""
        from datetime import datetime
        def log_debug(msg):
            with open("debug_log.txt", "a", encoding="utf-8") as f:
                f.write(f"{datetime.now()} [BRAIN]: {msg}\n")

        config = {"configurable": {"thread_id": chat_id or "1"}}
        inputs = {"messages": [HumanMessage(content=user_text)]}
        
        # Fresh DB Connection (Session State Cache)
        # RENAMED from memory.db to distinguish from Vector Memory
        db_path = "data/session_cache.db"
        os.makedirs("data", exist_ok=True)
        
        with sqlite3.connect(db_path, check_same_thread=False) as conn:
            memory = SqliteSaver(conn)
            app = self._build_graph(checkpointer=memory)
            
            last_tool_call = None
            
            log_debug(f"Starting stream for '{user_text}'")
            
            # REPETITION GUARD
            last_content_chunk = ""
            repetition_count = 0
            
            # STATEFUL THINKING PARSER
            # Tracks whether we're inside a <think>...</think> block
            in_thinking = False
            tag_buffer = ""  # Accumulates partial tags like "<", "<t", "<th", etc.
            
            # Auto-select model: if we've had 3+ consecutive 503s, use fallback
            if self._consecutive_503s >= 2:
                log_debug(f"Cloud model unstable ({self._consecutive_503s} failures). Using fallback: {self.fallback_model}")
                self.active_model = self.fallback_model
                # Rebuild graph with fallback LLM
                self.llm_with_tools = self.fallback_llm.bind_tools(self.tools)
                app = self._build_graph(checkpointer=memory)
            
            try:
                for msg, metadata in app.stream(inputs, config=config, stream_mode="messages"):
                    log_debug(f"Received msg type: {type(msg).__name__}, content_len={len(msg.content) if hasattr(msg, 'content') and msg.content else 0}, tool_chunks={bool(hasattr(msg, 'tool_call_chunks') and msg.tool_call_chunks)}")
                    
                    # 1. AI Message Chunk (Token)
                    if isinstance(msg, AIMessageChunk):
                        # Repetition Detection
                        if msg.content:
                            if msg.content == last_content_chunk:
                                repetition_count += 1
                                if repetition_count > 50: 
                                    log_debug("Repetition loop detected! Breaking.")
                                    break
                            else:
                                repetition_count = 0
                                last_content_chunk = msg.content
                                
                        # Check for tool call chunks
                        if msg.tool_call_chunks:
                            chunk = msg.tool_call_chunks[0]
                            
                            tool_name = None
                            if isinstance(chunk, dict):
                                tool_name = chunk.get("name")
                            elif hasattr(chunk, "name"):
                                tool_name = chunk.name
                                
                            if tool_name and tool_name != last_tool_call:
                                last_tool_call = tool_name
                                yield json.dumps({
                                    "type": "tool_start",
                                    "tool": last_tool_call,
                                    "args": {} 
                                }) + "\n"
                        
                        # Content Chunk (Real text response)
                        # NOTE: Use 'if' not 'elif' ‚Äî a chunk CAN have both
                        # tool_call_chunks AND content simultaneously
                        if msg.content and not msg.tool_call_chunks:
                            content = msg.content
                            log_debug(f"Content chunk ({len(content)} chars): '{content[:80]}...' | in_thinking={in_thinking}")
                            
                            # ===== STATEFUL THINK TAG PARSER =====
                            # Process character by character to handle tags
                            # split across multiple chunks
                            i = 0
                            while i < len(content):
                                char = content[i]
                                
                                # If we're accumulating a potential tag
                                if tag_buffer:
                                    tag_buffer += char
                                    
                                    # Check for complete <think> tag
                                    if tag_buffer == "<think>":
                                        in_thinking = True
                                        tag_buffer = ""
                                    # Check for complete </think> tag
                                    elif tag_buffer == "</think>":
                                        in_thinking = False
                                        tag_buffer = ""
                                    # Still a valid prefix of <think> or </think>?
                                    elif "<think>".startswith(tag_buffer) or "</think>".startswith(tag_buffer):
                                        pass  # Keep accumulating
                                    else:
                                        # Not a valid tag - flush buffer as content
                                        flush_content = tag_buffer
                                        tag_buffer = ""
                                        if in_thinking:
                                            yield json.dumps({
                                                "type": "thinking",
                                                "content": flush_content
                                            }) + "\n"
                                        else:
                                            yield json.dumps({
                                                "type": "response",
                                                "content": flush_content
                                            }) + "\n"
                                    i += 1
                                    continue
                                
                                # Start of a potential tag
                                if char == '<':
                                    tag_buffer = "<"
                                    i += 1
                                    continue
                                
                                # Normal content - emit based on state
                                if in_thinking:
                                    # Batch remaining non-tag content for thinking
                                    end = content.find('<', i + 1)
                                    if end == -1:
                                        end = len(content)
                                    yield json.dumps({
                                        "type": "thinking",
                                        "content": content[i:end]
                                    }) + "\n"
                                    i = end
                                else:
                                    # Batch remaining non-tag content for response
                                    end = content.find('<', i + 1)
                                    if end == -1:
                                        end = len(content)
                                    yield json.dumps({
                                        "type": "response",
                                        "content": content[i:end]
                                    }) + "\n"
                                    i = end

                    # 2. Tool Output Message (When tool finishes)
                    elif isinstance(msg, ToolMessage):
                        yield json.dumps({
                            "type": "tool_output",
                            "output": msg.content
                        }) + "\n"
                        
                # Flush any remaining tag buffer
                if tag_buffer:
                    event_type = "thinking" if in_thinking else "response"
                    yield json.dumps({
                        "type": event_type,
                        "content": tag_buffer
                    }) + "\n"
                
                # Success ‚Äî reset failure counter and restore primary model
                self._consecutive_503s = 0
                if self.active_model != self.primary_model:
                    log_debug(f"Response succeeded on fallback. Will try primary model next time.")
                    self.active_model = self.primary_model
                    self.llm_with_tools = self.llm.bind_tools(self.tools)
                    
            except Exception as e:
                error_str = str(e)
                log_debug(f"Stream Error: {error_str}")
                
                # Track 503/500 errors for auto-fallback
                if "503" in error_str or "500" in error_str or "Service Temporarily Unavailable" in error_str:
                    self._consecutive_503s += 1
                    log_debug(f"503 count: {self._consecutive_503s}")
                    
                    # On first failure, retry once with fallback model immediately
                    if self._consecutive_503s <= 2:
                        log_debug(f"Retrying with fallback model: {self.fallback_model}")
                        yield json.dumps({"type": "status", "content": f"Cloud model unavailable, switching to {self.fallback_model}..."}) + "\n"
                        
                        try:
                            self.llm_with_tools = self.fallback_llm.bind_tools(self.tools)
                            retry_app = self._build_graph(checkpointer=memory)
                            
                            for msg, metadata in retry_app.stream(inputs, config=config, stream_mode="messages"):
                                if isinstance(msg, AIMessageChunk) and msg.content:
                                    yield json.dumps({"type": "response", "content": msg.content}) + "\n"
                                elif isinstance(msg, ToolMessage):
                                    yield json.dumps({"type": "tool_output", "output": msg.content}) + "\n"
                            
                            # Retry succeeded
                            self._consecutive_503s = 0
                            log_debug("Fallback retry succeeded!")
                        except Exception as retry_e:
                            log_debug(f"Fallback retry also failed: {retry_e}")
                            yield json.dumps({"type": "error", "content": f"Both models failed. Error: {retry_e}"}) + "\n"
                    else:
                        yield json.dumps({"type": "error", "content": f"Cloud model keeps failing ({self._consecutive_503s}x). Using fallback for next messages."}) + "\n"
                else:
                    yield json.dumps({"type": "error", "content": error_str}) + "\n"

    # ==================== AUTONOMOUS EXECUTION ====================
    
    def run_autonomous_task(self, objective: str) -> str:

        """
        Runs the agent loop for a specific objective purely autonomously.
        No user input is involved. Nexus uses its tools to achieve the objective.
        
        Args:
            objective: The task description (e.g., "Research Quantum Computing")
            
        Returns:
            The final result/summary of the work.
        """
        from langchain_core.messages import SystemMessage, HumanMessage
        
        # 1. Create a special system prompt for work mode
        work_prompt = f"""You are Nexus in **Deep Work Mode** ‚Äî fully autonomous goal execution.
        
        üéØ YOUR GOAL: {objective}
        
        EXECUTION RULES (MANDATORY):
        1. DECOMPOSE the goal into numbered steps FIRST.
        2. Execute each step using tools (search, shell, etc.).
        3. After EVERY tool call, check: "Did this move me closer to the goal?"
        4. NEVER observe without acting ‚Äî if you call see_screen, your next call MUST be an action.
        5. NEVER retry the same failed action more than 2 times ‚Äî adapt or report.
        6. Do NOT ask the user for help. You are working autonomously.
        7. When complete, output results in this format:
           ‚úÖ GOAL COMPLETE: [summary]
           üìä Results: [specifics]
        8. Be concise and professional. No filler, no narration.
        """
        
        # 2. Initialize ephemeral graph for this task (stateless or separate thread)
        # We use a fresh connection to avoid messing with the main chat history
        workflow = StateGraph(MessagesState)
        workflow.add_node("agent", self.call_model)
        workflow.add_node("tools", ToolNode(self.tools))
        workflow.add_edge(START, "agent")
        workflow.add_conditional_edges("agent", tools_condition)
        workflow.add_edge("tools", "agent")
        app = workflow.compile()
        
        # 3. Run the loop
        messages = [SystemMessage(content=work_prompt), HumanMessage(content=f"Start working on: {objective}")]
        final_response = ""
        
        try:
            print(f"[Brain] üß† Starting Deep Work: {objective}")
            # Run for a maximum of 10 steps to prevent infinite loops
            for event in app.stream({"messages": messages}, stream_mode="values", config={"recursion_limit": 15}):
                last_msg = event["messages"][-1]
                if isinstance(last_msg, AIMessage) and not last_msg.tool_calls:
                     final_response = last_msg.content
            
            return final_response
        except Exception as e:
            return f"I tried to work on {objective} but encountered an error: {e}"